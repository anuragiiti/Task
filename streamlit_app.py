# -*- coding: utf-8 -*-
"""streamlit_app.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uSIg5t9yOlwjo1sErIC354gdWF66cck1
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder

# Load datasets
train_dataset = pd.read_excel('train.xlsx')
test_dataset = pd.read_excel('test.xlsx')

# KMeans clustering
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(train_dataset.iloc[:, :-1].values)
    wcss.append(kmeans.inertia_)

# Number of clusters found is 4
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans.fit_predict(train_dataset.iloc[:, :-1].values)

# Sidebar for user input
user_data_point_str = st.sidebar.text_input("Enter data point (comma-separated):", "")
if user_data_point_str:
    user_data_point = list(map(float, user_data_point_str.split(',')))
    user_data_point = [user_data_point]

    # Predict cluster
    predicted_cluster = kmeans.predict(user_data_point)

    # Display result
    st.write(f"The data point {user_data_point} belongs to Cluster {predicted_cluster[0]}")

# Load datasets for Task-2
X_train_fs = StandardScaler().fit_transform(train_dataset.iloc[:, :-1].values)
X_test_fs = StandardScaler().fit_transform(test_dataset.iloc[:, :].values)

# SVM model
classifier_svm = SVC(kernel='rbf', random_state=0)
classifier_svm.fit(X_train_fs, train_dataset.iloc[:, -1].values)

# Naive Bayes model
classifier_nb = GaussianNB()
classifier_nb.fit(X_train_fs, train_dataset.iloc[:, -1].values)

# Random Forest model
classifier_rf = RandomForestClassifier(n_estimators=30, criterion='entropy', random_state=0)
classifier_rf.fit(X_train_fs, train_dataset.iloc[:, -1].values)

# Neural Network model
ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
y_train_le = LabelEncoder().fit_transform(train_dataset.iloc[:, -1].values)
ann.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
ann.fit(train_dataset.iloc[:, :-1].values, y_train_le, batch_size=100, epochs=100)

# User input for Task-2
user_data_point_str_2 = st.sidebar.text_input("Enter data point for Task-2 (comma-separated):", "")
if user_data_point_str_2:
    user_data_point_2 = list(map(float, user_data_point_str_2.split(',')))
    user_data_point_2 = [user_data_point_2]

    # Predictions
    y_pred_svm = classifier_svm.predict([user_data_point_2])
    y_pred_nb = classifier_nb.predict([user_data_point_2])
    y_pred_rf = classifier_rf.predict([user_data_point_2])
    y_pred_nn = ann.predict([user_data_point_2])
    y_pred_nn = (y_pred_nn > 0.5)

    # Display results
    st.write("Task 2:")
    st.write(f"SVM Prediction: {y_pred_svm[0]}")
    st.write(f"Naive Bayes Prediction: {y_pred_nb[0]}")
    st.write(f"Random Forest Prediction: {y_pred_rf[0]}")
    st.write(f"Neural Network Prediction: {y_pred_nn[0]}")

# Load dataset for Task-3
dataset = pd.read_excel('rawdata.xlsx')
dataset['datetime'] = pd.to_datetime(dataset['date'])
dataset = dataset.drop(['date', 'time'], axis=1)

# Filtering Rows With 'placed' Activity For Inside And Outside
inside_placed = dataset[(dataset['activity'] == 'placed') & (dataset['position'].str.lower() == 'inside')]
outside_placed = dataset[(dataset['activity'] == 'placed') & (dataset['position'].str.lower() == 'outside')]

# Calculating Date-Wise Total Duration For Inside And Outside
inside_duration = inside_placed.groupby(inside_placed['datetime'].dt.date)['number'].count()
outside_duration = outside_placed.groupby(outside_placed['datetime'].dt.date)['number'].count()

# Filtering Rows With 'picked' Activity For Inside And Outside
inside_picked = dataset[(dataset['activity'] == 'picked') & (dataset['position'].str.lower() == 'inside')]
outside_picked = dataset[(dataset['activity'] == 'picked') & (dataset['position'].str.lower() == 'outside')]

# Calculating Date-Wise Number Of Picking Activities For Inside And Outside
inside_picking_count = inside_picked.groupby(inside_picked['datetime'].dt.date)['number'].count()
outside_picking_count = outside_picked.groupby(outside_picked['datetime'].dt.date)['number'].count()

# Display results for Task 3
st.write("Task 3:")
st.write("Date-wise Total Duration for Inside:")
st.write(inside_duration)

st.write("Date-wise Total Duration for Outside:")
st.write(outside_duration)

st.write("Date-wise Number of Picking Activities for Inside:")
st.write(inside_picking_count)

st.write("Date-wise Number of Picking Activities for Outside:")
st.write(outside_picking_count)